<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine_learning - Jan Meppe</title>
    <link>https://www.janmeppe.com/notes/machine_learning/index.xml</link>
    <description></description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Apr 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://www.janmeppe.com/notes/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Accuracy</title>
      <link>https://www.janmeppe.com/notes/machine_learning/model_evaluation/accuracy/</link>
      <pubDate>Wed, 14 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/model_evaluation/accuracy/</guid>
      <description>If you want to calculate the accuracy with cross-validation, check out this.
Imports from sklearn.metrics import accuracy_score Create data y_true = [0, 1, 2, 3, 4] y_pred = [0, 2, 1, 3, 5] Calculate accuracy accuracy_score(y_true, y_pred) 0.4  </description>
    </item>
    
    <item>
      <title>Get n&#39;th row of dataframe</title>
      <link>https://www.janmeppe.com/notes/machine_learning/vectors_matrices_and_arrays/get-nth-row-of-dataframe/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/vectors_matrices_and_arrays/get-nth-row-of-dataframe/</guid>
      <description>Imports # Load library import pandas as pd Create data df = pd.DataFrame({ &amp;#34;apples&amp;#34;: [1, 1], &amp;#34;oranges&amp;#34;: [2, 4] }) Get dataframe with .iloc[[]] df.iloc[[0]]  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  apples oranges     0 1 2     Get series with iloc[] df.</description>
    </item>
    
    <item>
      <title>Iris pipeline</title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/iris_pipeline/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/iris_pipeline/</guid>
      <description>Imports from sklearn import datasets import numpy as np import pandas as pd import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV Load data iris = datasets.load_iris() df = pd.DataFrame( data=np.c_[iris[&amp;#34;data&amp;#34;], iris[&amp;#34;target&amp;#34;]], columns=iris[&amp;#34;feature_names&amp;#34;] + [&amp;#34;target&amp;#34;] ) df.head()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</description>
    </item>
    
    <item>
      <title>Combining columns together</title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipelines/combining_columns_together/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipelines/combining_columns_together/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer Create data data = {&amp;#39;label&amp;#39;: [&amp;#39;dog&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;catdog&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;catdog&amp;#39;], &amp;#39;score&amp;#39;: [1, 2, 3, 4, 5]} df = pd.DataFrame(data, columns = [&amp;#34;label&amp;#34;, &amp;#34;score&amp;#34;]) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</description>
    </item>
    
    <item>
      <title>Convert categorical data to labels</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/convert_categorical_data_to_labels/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/convert_categorical_data_to_labels/</guid>
      <description>LabelEncoder is just another scikit-learn estimator with a fit() method and a transform() method.
Imports import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder Create data data = {&amp;#39;label&amp;#39;: [&amp;#39;dog&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;catdog&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;catdog&amp;#39;]} df = pd.DataFrame(data, columns = [&amp;#34;label&amp;#34;]) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</description>
    </item>
    
    <item>
      <title>Create simulated data for clustering</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_for_clustering_blobs/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_for_clustering_blobs/</guid>
      <description>Create simulated data for clustering (blobs) Imports from sklearn.datasets import make_blobs import matplotlib.pyplot as plt import numpy as np Create data # Make features (data) and label (labels) data, labels = make_blobs(n_samples = 200, n_features = 2, centers = 3, # 3 clusters cluster_std = 0.5, shuffle =True) Set up colours from itertools import islice, cycle color_list = [&amp;#39;#377eb8&amp;#39;, &amp;#39;#ff7f00&amp;#39;, &amp;#39;#4daf4a&amp;#39;, &amp;#39;#f781bf&amp;#39;, &amp;#39;#a65628&amp;#39;, &amp;#39;#984ea3&amp;#39;, &amp;#39;#999999&amp;#39;, &amp;#39;#e41a1c&amp;#39;, &amp;#39;#dede00&amp;#39;] amount_of_labels = int(max(labels) + 1) colors = np.</description>
    </item>
    
    <item>
      <title>Create simulated data for regression</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_for_regression/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_for_regression/</guid>
      <description>Creating simulated data for regression Generate a problem set for regression with make_regression.
Imports import pandas as pd from sklearn.datasets import make_regression Create data n_samples = 100 n_features = 1 X, y, coeff = make_regression(n_samples=n_samples, n_features=n_features, noise = 10, coef=True) View data X array([[ 0.39342141], [-0.29941625], [ 0.8270667 ], [-0.36925149], [ 0.14036599], [-0.86022152], [-0.23233681], [ 2.37128331], [-0.67873926], [-0.29949625], [-0.90195881], [-1.28719771], [-1.26405413], [ 0.27266678], [ 0.16784455], [-0.20048219], [-1.42941065], [ 0.60842469], [-0.48417899], [-1.</description>
    </item>
    
    <item>
      <title>Create simulated data moons</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_moons/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/create_simulated_data_moons/</guid>
      <description>Create simulate data moons Imports from itertools import cycle, islice from sklearn.datasets import make_moons Create data Generate X and y where X is the position and label is the label.
X, labels = make_moons(noise=0.1) Generate colormap (what the fuck?) colors = np.array(list(islice(cycle([&amp;#39;#377eb8&amp;#39;, &amp;#39;#ff7f00&amp;#39;, &amp;#39;#4daf4a&amp;#39;, &amp;#39;#f781bf&amp;#39;, &amp;#39;#a65628&amp;#39;, &amp;#39;#984ea3&amp;#39;, &amp;#39;#999999&amp;#39;, &amp;#39;#e41a1c&amp;#39;, &amp;#39;#dede00&amp;#39;]), int(max(label) + 1)))) View data import matplotlib.pyplot as plt x = X[:,0] y = X[:,1] plt.scatter(x, y, color=colors[labels]) plt.show() </description>
    </item>
    
    <item>
      <title>Discretize features</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/discretize_features/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/discretize_features/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.preprocessing import Binarizer Create data d = {&amp;#39;values&amp;#39;: [6, 10, 12, 100]} df = pd.DataFrame(d) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  values     0 6   1 10   2 12   3 100     Option 1: Binarize into 0/1 binarizer = Binarizer(10) binarizer.</description>
    </item>
    
    <item>
      <title>Drop rows missing data</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/drop_rows_missing_data/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/drop_rows_missing_data/</guid>
      <description>Imports import pandas as pd import numpy as np Create data d = {&amp;#39;col1&amp;#39;: [1, np.nan, 10, 14], &amp;#39;col2&amp;#39;: [3, 4, 5, np.nan]} df = pd.DataFrame(d) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  col1 col2     0 1.0 3.0   1 NaN 4.0   2 10.</description>
    </item>
    
    <item>
      <title>Encode ordinal categorical</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/encode_ordinal_categorical_features/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/encode_ordinal_categorical_features/</guid>
      <description>OrdinalEncoder is very confusing (so don&amp;rsquo;t worry if you don&amp;rsquo;t get it&amp;hellip; it&amp;rsquo;s confusing) Make sure you input a list of lists  import pandas as pd import numpy as np from sklearn.preprocessing import OrdinalEncoder Create data d = {&amp;#39;rating&amp;#39;: [&amp;#34;first&amp;#34;, &amp;#34;second&amp;#34;, &amp;#34;third&amp;#34;, &amp;#34;first&amp;#34;, &amp;#34;second&amp;#34;, &amp;#34;second&amp;#34;]} df = pd.DataFrame(d) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</description>
    </item>
    
    <item>
      <title>Impute missing data</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/impute-missing-data/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/impute-missing-data/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.impute import SimpleImputer Create data d = {&amp;#39;col1&amp;#39;: [1, np.nan, 10, 14], &amp;#39;col2&amp;#39;: [3, 4, 5, np.nan]} df = pd.DataFrame(d) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  col1 col2     0 1.0 3.0   1 NaN 4.</description>
    </item>
    
    <item>
      <title>Loading keras imdb dataset</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/loading_keras_imdb_dataset/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/loading_keras_imdb_dataset/</guid>
      <description>Loading Keras IMDB dataset import numpy as np import pandas as pd from keras.datasets import imdb from keras.preprocessing.text import Tokenizer This downloads 25.000 movie reviews from IMDB with the label positive/negative.
Each review is encoded as a list of indexes (integers)
Words are encoded by overall frequency in the dataset. Integer 3 encodes the 3rd most frequent word in the data.
Create data # set number of features (use top 1000 words) num_words = 1000 index_from = 3 # load data (x_train, y_train), (x_test, y_test) = imdb.</description>
    </item>
    
    <item>
      <title>Loading scikit learn iris dataset</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_iris_dataset/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_iris_dataset/</guid>
      <description>Loading scikit-learn Iris dataset from sklearn import datasets import numpy as np import pandas as pd Load Iris Dataset The Iris flower dataset is one of the most famous databases for classification.
The dataset contains:
 3 classes (species of flowers) 50 observations per class  # Load Iris datset iris = datasets.load_iris() dir(iris) [&#39;DESCR&#39;, &#39;data&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;target&#39;, &#39;target_names&#39;]  # Create features X = iris.data # Create label y = iris.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn boston housing dataset</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_boston_housing_dataset/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_boston_housing_dataset/</guid>
      <description>Loading scikit-learn Boston housing dataset import numpy as np import pandas as pd from sklearn import datasets Load Boston Housing Dataset The Boston housing dataset is a famous dataset from the 1970s. It contains 506 observations on housing prices around Boston. It is often used in regression examples and contains 15 features.
boston = datasets.load_boston() # Load features X = boston.data # Load data y = boston.target # View first observation X[0] array([6.</description>
    </item>
    
    <item>
      <title>Loading scikit-learn digits dataset</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_digits_dataset/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/loading_scikit_learn_digits_dataset/</guid>
      <description>Loading scikit-learn digits dataset from sklearn import datasets import matplotlib.pyplot as plt Load digits dataset The Digits dataset is a dataset of handwritten digits. Each feature is the intensity of a single pixel in an 8x8 image.
# Load digits = datasets.load_digits() # View attributes dir(digits) [&#39;DESCR&#39;, &#39;data&#39;, &#39;images&#39;, &#39;target&#39;, &#39;target_names&#39;]  # Create features X = digits.data # Create target label y = digits.target Each of the futures are represented as a vector of length 64 instead of an 8x8 image.</description>
    </item>
    
    <item>
      <title>One hot encode categorical features</title>
      <link>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/one_hot_encode_categorical_features/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/preprocessing_structured_data/one_hot_encode_categorical_features/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder Create data d = {&amp;#39;fruit&amp;#39;: [&amp;#39;apple&amp;#39;, &amp;#39;pear&amp;#39;, &amp;#39;apple&amp;#39;, &amp;#39;pear&amp;#39;, &amp;#39;pear&amp;#39;]} df = pd.DataFrame(d) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  fruit     0 apple   1 pear   2 apple   3 pear   4 pear     Initialise one_hot = OneHotEncoder() Train one_hot.</description>
    </item>
    
    <item>
      <title>Selecting non-numerical columns</title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipelines/selecting_non_numerical_columns/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipelines/selecting_non_numerical_columns/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline Create data data = {&amp;#39;label&amp;#39;: [&amp;#39;dog&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;catdog&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;catdog&amp;#39;], &amp;#39;score&amp;#39;: [1, 2, 3, 4, 5]} df = pd.DataFrame(data, columns = [&amp;#34;label&amp;#34;, &amp;#34;score&amp;#34;]) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  label score     0 dog 1   1 cat 2   2 catdog 3   3 dog 4   4 catdog 5     Define numerical columns def get_non_numerical_columns(df): numerics = list(df.</description>
    </item>
    
    <item>
      <title>Selecting numerical columns</title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipelines/selecting_numerical_columns/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipelines/selecting_numerical_columns/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline Create data data = {&amp;#39;label&amp;#39;: [&amp;#39;dog&amp;#39;, &amp;#39;cat&amp;#39;, &amp;#39;catdog&amp;#39;, &amp;#39;dog&amp;#39;, &amp;#39;catdog&amp;#39;], &amp;#39;score&amp;#39;: [1, 2, 3, 4, 5]} df = pd.DataFrame(data, columns = [&amp;#34;label&amp;#34;, &amp;#34;score&amp;#34;]) df  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  
  label score     0 dog 1   1 cat 2   2 catdog 3   3 dog 4   4 catdog 5     Define numerical columns numerical = list(df.</description>
    </item>
    
    <item>
      <title>Should you train preprocessing on the test set?</title>
      <link>https://www.janmeppe.com/notes/machine_learning/basics/should_you_train_preprocessing_on_test_set/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/basics/should_you_train_preprocessing_on_test_set/</guid>
      <description>NO. NEVER PREPROCESS YOUR TEST SET
This is a mistake because it leaks data from your train set into your test set.
Consider this example, first a processing routine is applied:
def processing(df): ... return(df) df = processing(df) And then later the data is split into a test and train set:
X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42) This is wrong, and only right by accident.
Do this the other way around.</description>
    </item>
    
    <item>
      <title>Simple Boston pipeline</title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/simple_boston_pipeline/</link>
      <pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/simple_boston_pipeline/</guid>
      <description>Imports import pandas as pd import numpy as np from sklearn.datasets import load_boston from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import Ridge from sklearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV Load in data data = load_boston() X_train, X_test, y_train, y_test = train_test_split(data[&amp;#39;data&amp;#39;], data[&amp;#39;target&amp;#39;]) Create pipeline pipe = Pipeline([ (&amp;#39;scaler&amp;#39;, StandardScaler()), (&amp;#39;reduce_dim&amp;#39;, PCA()), (&amp;#39;regressor&amp;#39;, Ridge()) ]) Fit pipeline pipe = pipe.fit(X_train, y_train) View parameters with get_params() pipe.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.janmeppe.com/notes/machine_learning/cases/bot-or-not-v1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/cases/bot-or-not-v1/</guid>
      <description>Idea parking space:
 Extract features from URL  What I&amp;rsquo;ve done so far:
 Load in the data Inspect the data Look for unique values with (df.value_counts) Look for missing values with df.info Drop na values (todo: refactor this in pipeline) Create targets and remove from df  import pandas as pd import numpy as np import os filename = &amp;#34;../../../data/bot-or-not-clickdata.csv&amp;#34; df = pd.read_csv(filename) Data:
 epoch_ms session_id country_by_ip_address region_by_ip_address url_without_parameters referrer_without_parameters visitor_recognition_type ua_agent_class  df.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.janmeppe.com/notes/machine_learning/cases/bot-or-not-v2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/cases/bot-or-not-v2/</guid>
      <description>Idea parking space:
 Extract features from URL  What I&amp;rsquo;ve done so far:
 Load in the data Inspect the data Look for unique values with (df.value_counts) Look for missing values with df.info Drop na values (todo: refactor this in pipeline) Create targets and remove from df  Bot or not v2 This is version 2 of the bot or not framework where we try to incorporate more features and try to put everything in a single pipeline.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/untitled/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://www.janmeppe.com/notes/machine_learning/pipeline_examples/untitled/</guid>
      <description>from sklearn.datasets import fetch_20newsgroups categories = [&amp;#34;alt.atheism&amp;#34;, &amp;#34;talk.religion.misc&amp;#34;] data = fetch_20newsgroups(categories=categories) data Downloading 20news dataset. This may take a few minutes. Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB) {&#39;data&#39;: [&#39;From: mangoe@cs.umd.edu (Charley Wingate)\nSubject: Benediktine Metaphysics\nLines: 24\n\nBenedikt Rosenau writes, with great authority:\n\n&amp;gt; IF IT IS CONTRADICTORY IT CANNOT EXIST.\n\n&amp;quot;Contradictory&amp;quot; is a property of language. If I correct this to\n\n\n THINGS DEFINED BY CONTRADICTORY LANGUAGE DO NOT EXIST\n\nI will object to definitions as reality.</description>
    </item>
    
  </channel>
</rss>